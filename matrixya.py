import aiohttp
import asyncio
import aiofiles
import json
import uvicorn
import logging
import time
import os
import numpy as np
import re
import pickle
from pathlib import Path
from fastapi import FastAPI, HTTPException, Form, UploadFile, File
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from voicerecognise import recognize_audio_with_sdk
from yandex_cloud_ml_sdk import YCloudML
from typing import Dict, List, Optional
import faiss  

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

BASE_DIR = "base"
EMBEDDINGS_DIR = "embeddings_data"
os.makedirs(BASE_DIR, exist_ok=True)
os.makedirs(EMBEDDINGS_DIR, exist_ok=True)
API_URL = "https://dev.back.matrixcrm.ru/api/v1/AI/servicesByFilters"

YANDEX_FOLDER_ID = "b1gnq2v60fut60hs9vfb"
YANDEX_API_KEY = "AQVNw5Kg0jXoaateYQWdSr2k8cbst_y4_WcbvZrW"

logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
search_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
logger.info("–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

conversation_history: Dict[str, Dict] = {}

app = FastAPI()


def get_tenant_path(tenant_id: str) -> Path:
    """–°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫—É –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–∞–Ω—Ç–∞"""
    tenant_path = Path(EMBEDDINGS_DIR) / tenant_id
    tenant_path.mkdir(parents=True, exist_ok=True)
    return tenant_path


def normalize_text(text: str) -> str:
    # –°–æ—Ö—Ä–∞–Ω–∏–º –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫, —É–¥–∞–ª—è—è –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ \n
    text = text.strip()
    text = re.sub(r"[^\w\s\d\n]", "", text)
    return text.lower()


def tokenize_text(text: str) -> List[str]:
    stopwords = {
        "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–∫–∞–∫", "—á—Ç–æ", "—ç—Ç–æ", "–Ω–æ",
        "–∞", "–∏–ª–∏", "—É", "–æ", "–∂–µ", "–∑–∞", "–∫", "–∏–∑", "–æ—Ç", "—Ç–∞–∫", "—Ç–æ", "–≤—Å–µ"
    }
    tokens = text.split()
    return [word for word in tokens if word not in stopwords]


def extract_text_fields(record: dict) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏, —Å–æ—Ö—Ä–∞–Ω—è—é—â–µ–µ –∏–µ—Ä–∞—Ä—Ö–∏—é:
      –§–∏–ª–∏–∞–ª: <filialName>
      –ö–∞—Ç–µ–≥–æ—Ä–∏—è: <categoryName>
      –£—Å–ª—É–≥–∞: <serviceName>
      –û–ø–∏—Å–∞–Ω–∏–µ —É—Å–ª—É–≥–∏: <serviceDescription>
      –¶–µ–Ω–∞: <price>
      –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: <employeeFullName>
      –û–ø–∏—Å–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: <employeeDescription>
    """
    filial = record.get("filialName", "–§–∏–ª–∏–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω")
    category = record.get("categoryName", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞")
    service = record.get("serviceName", "–£—Å–ª—É–≥–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞")
    service_desc = record.get("serviceDescription", "–û–ø–∏—Å–∞–Ω–∏–µ —É—Å–ª—É–≥–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ")
    price = record.get("price", "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞")
    specialist = record.get("employeeFullName", "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –Ω–µ —É–∫–∞–∑–∞–Ω")
    spec_desc = record.get("employeeDescription", "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ")
    text = (
        f"–§–∏–ª–∏–∞–ª: {filial}\n"
        f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n"
        f"–£—Å–ª—É–≥–∞: {service}\n"
        f"–û–ø–∏—Å–∞–Ω–∏–µ —É—Å–ª—É–≥–∏: {service_desc}\n"
        f"–¶–µ–Ω–∞: {price}\n"
        f"–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: {specialist}\n"
        f"–û–ø–∏—Å–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: {spec_desc}"
    )
    return normalize_text(text)


async def load_json_data(tenant_id: str) -> List[dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON-—Ñ–∞–π–ª–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π.
    """
    file_path = os.path.join(BASE_DIR, f"{tenant_id}.json")
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"–§–∞–π–ª –¥–ª—è tenant_id={tenant_id} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    
    async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
        content = await f.read()
        data = json.loads(content)
    
    records = []
    branches = data.get("data", {}).get("branches", [])
    for branch in branches:
        filial_name = branch.get("name", "–§–∏–ª–∏–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω")
        categories = branch.get("categories", [])
        for category in categories:
            category_name = category.get("name", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞")
            services = category.get("services", [])
            for service in services:
                service_name = service.get("name", "–£—Å–ª—É–≥–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞")
                price = service.get("price", "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞")
                service_description = service.get("description", "")
                employees = service.get("employees", [])
                if employees:
                    for emp in employees:
                        employee_full_name = emp.get("full_name", "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –Ω–µ —É–∫–∞–∑–∞–Ω")
                        employee_description = emp.get("description", "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ")
                        record = {
                            "filialName": filial_name,
                            "categoryName": category_name,
                            "serviceName": service_name,
                            "serviceDescription": service_description,
                            "price": price,
                            "employeeFullName": employee_full_name,
                            "employeeDescription": employee_description
                        }
                        records.append(record)
                else:
                    record = {
                        "filialName": filial_name,
                        "categoryName": category_name,
                        "serviceName": service_name,
                        "serviceDescription": service_description,
                        "price": price,
                        "employeeFullName": "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –Ω–µ —É–∫–∞–∑–∞–Ω",
                        "employeeDescription": "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ"
                    }
                    records.append(record)
    return records


async def prepare_data(tenant_id: str):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–Ω–∞–Ω—Ç–∞: –∑–∞–≥—Ä—É–∂–∞–µ—Ç JSON, —Å—Ç—Ä–æ–∏—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, BM25 –∏ FAISS-–∏–Ω–¥–µ–∫—Å.
    """
    tenant_path = get_tenant_path(tenant_id)
    data_file = tenant_path / "data.json"
    embeddings_file = tenant_path / "embeddings.npy"
    bm25_file = tenant_path / "bm25.pkl"
    faiss_index_file = tenant_path / "faiss_index.index"
    
    if all([f.exists() for f in [data_file, embeddings_file, bm25_file, faiss_index_file]]):
        file_age = time.time() - os.path.getmtime(data_file)
        if file_age < 2_592_000:
            async with aiofiles.open(data_file, "r", encoding="utf-8") as f:
                data = json.loads(await f.read())
            embeddings = np.load(embeddings_file)
            with open(bm25_file, "rb") as f:
                bm25 = pickle.load(f)
            index = faiss.read_index(str(faiss_index_file))
            return data, embeddings, bm25, index

    records = await load_json_data(tenant_id)
    documents = [extract_text_fields(record) for record in records]

    loop = asyncio.get_event_loop()
    embeddings, bm25 = await asyncio.gather(
        loop.run_in_executor(None, lambda: search_model.encode(documents, convert_to_tensor=True).cpu().numpy()),
        loop.run_in_executor(None, lambda: BM25Okapi([tokenize_text(doc) for doc in documents]))
    )

    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    faiss.write_index(index, str(faiss_index_file))

    async with aiofiles.open(data_file, "w", encoding="utf-8") as f:
        await f.write(json.dumps({
            "records": records,
            "raw_texts": documents,
            "timestamp": time.time()
        }, ensure_ascii=False, indent=4))

    np.save(embeddings_file, embeddings)
    with open(bm25_file, "wb") as f:
        pickle.dump(bm25, f)

    return {"records": records, "raw_texts": documents}, embeddings, bm25, index


async def update_json_file(mydtoken: str, tenant_id: str):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ, –∑–∞–ø—Ä–∞—à–∏–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è—Å—å –ø–µ—Ä–≤—ã–º–∏ 50 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏.
    """
    tenant_path = get_tenant_path(tenant_id)
    file_path = os.path.join(BASE_DIR, f"{tenant_id}.json")
    
    if os.path.exists(file_path):
        file_age = time.time() - os.path.getmtime(file_path)
        if file_age < 2_592_000:
            logger.info(f"–§–∞–π–ª {file_path} –∞–∫—Ç—É–∞–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
            return
    
    for f in tenant_path.glob("*"):
        try:
            os.remove(f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {f}: {e}")
    
    try:
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"Bearer {mydtoken}"}
            params = {"tenantId": tenant_id, "page": 1}
            all_data = []
            max_pages = 500
            while True:
                if params["page"] > max_pages:
                    logger.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü, –∑–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É.")
                    break
                async with session.get(API_URL, headers=headers, params=params) as response:
                    response.raise_for_status()
                    data = await response.json()
                    branches = data.get("data", {}).get("branches", [])
                    if not branches:
                        logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {params['page']} –ø—É—Å—Ç–∞—è, –∑–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É.")
                        break
                    all_data.extend(branches)
                    logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(branches)} –∑–∞–ø–∏—Å–µ–π —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {params['page']}.")
                    params["page"] += 1

            logger.info(f"–û–±—â–µ–µ —á–∏—Å–ª–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Ñ–∏–ª–∏–∞–ª–æ–≤: {len(all_data)}")
            async with aiofiles.open(file_path, "w", encoding="utf-8") as json_file:
                await json_file.write(json.dumps(
                    {"code": data.get("code", 200), "data": {"branches": all_data}},
                    ensure_ascii=False,
                    indent=4
                ))
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.")


async def rerank_with_cross_encoder(query: str, candidates: List[int], raw_texts: List[str]) -> List[int]:
    """–†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Ç–æ–ø-10 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–∞"""
    cross_inp = [(query, raw_texts[idx]) for idx in candidates]
    loop = asyncio.get_event_loop()
    cross_scores = await loop.run_in_executor(None, lambda: cross_encoder.predict(cross_inp))
    sorted_indices = np.argsort(cross_scores)[::-1].tolist()
    return [candidates[i] for i in sorted_indices]



free_times_function = {
    "name": "getFreeTimesOfEmployeeByChoosenServices",
    "description": "–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–±–æ–¥–Ω–æ–µ –≤—Ä–µ–º—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —É—Å–ª—É–≥–∞–º",
    "parameters": {
        "type": "object",
        "properties": {
            "employeeId": {"type": "string", "description": "ID —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞"},
            "serviceId": {"type": "array", "items": {"type": "string"}, "description": "–°–ø–∏—Å–æ–∫ ID —É—Å–ª—É–≥"},
            "dateTime": {"type": "string", "description": "–î–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD"},
            "tenantId": {"type": "string", "description": "ID —Ç–µ–Ω–∞–Ω—Ç–∞"},
            "filialId": {"type": "string", "description": "ID —Ñ–∏–ª–∏–∞–ª–∞"},
            "langId": {"type": "string", "description": "–Ø–∑—ã–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ru)"}
        },
        "required": ["employeeId", "serviceId", "dateTime", "tenantId", "filialId", "langId"]
    }
}


async def generate_yandexgpt_response(context: str, history: List[dict], question: str) -> str:
    """
    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º YandexGPT.
    """
    system_prompt = """# üîπ –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ üîπ

–¢—ã ‚Äì –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–ª–∏–Ω–∏–∫–∏ MED YU MED –ø–æ –∏–º–µ–Ω–∏ –ê–∏–¥–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äì –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –Ω–∞—Ö–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞—Ö, —É—Å–ª—É–≥–∞—Ö, —Ñ–∏–ª–∏–∞–ª–∞—Ö –∏ —Ü–µ–Ω–∞—Ö. –¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –∫–∞–∫ RAG-–º–æ–¥–µ–ª—å (Retrieval-Augmented Generation), —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ:

1. –í–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Äì –≤ –Ω—ë–º –µ—Å—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞—Ö, —É—Å–ª—É–≥–∞—Ö, —Ü–µ–Ω–∞—Ö –∏ —Ñ–∏–ª–∏–∞–ª–∞—Ö. –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.

2. –¢–µ–±–µ –Ω–µ –Ω—É–∂–Ω–æ –≤—ã–¥—É–º—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é ‚Äì –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –ø—Ä—è–º–æ —Å–æ–æ–±—â–∞–π –æ–± —ç—Ç–æ–º. 

3. –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É—Ç–æ—á–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å, —Ç—ã –æ–±—è–∑–∞–Ω–∞ –∏—Å–∫–∞—Ç—å –æ—Ç–≤–µ—Ç –∏–º–µ–Ω–Ω–æ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞. 

## üìå 1. –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞

- –ü—Ä–æ—á–∏—Ç–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø–æ–π–º–∏, –∫ —á–µ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∑–∞–ø—Ä–æ—Å: —É—Å–ª—É–≥–∏, —Ü–µ–Ω—ã, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã, —Ñ–∏–ª–∏–∞–ª—ã –∏ —Ç. –¥.
- –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω ‚Äì –∑–∞–¥–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –¥–æ–≥–∞–¥—ã–≤–∞—Ç—å—Å—è.

## üîç 2. –ü–æ–¥–±–æ—Ä —É—Å–ª—É–≥

–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ —É—Å–ª—É–≥–∏, —Ç—ã –æ–±—è–∑–∞–Ω–∞:
- –ù–∞–π—Ç–∏ –≤—Å–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —É—Å–ª—É–≥–∏.
- –£–∫–∞–∑–∞—Ç—å —Ü–µ–Ω—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–æ—Ç 12000 —Ä—É–±–ª–µ–π üí∏").
- –ù–∞–∑–≤–∞—Ç—å —Ñ–∏–ª–∏–∞–ª, –≥–¥–µ –¥–æ—Å—Ç—É–ø–Ω–∞ —É—Å–ª—É–≥–∞ (–ú–æ—Å–∫–≤–∞ ‚Äì –•–æ–¥—ã–Ω–∫–∞, –ú–æ—Å–∫–≤–∞ ‚Äì –°–∏—Ç–∏, –î—É–±–∞–π).
- –ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç—å –≤—Å–µ—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç —ç—Ç—É —É—Å–ª—É–≥—É (–±–µ–∑ —Å–ª–æ–≤ "–∏ –¥—Ä—É–≥–∏–µ", —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–µ —Å–ø–∏—Å–∫–∏!).
- –û–±—ä—è—Å–Ω–∏—Ç—å –ø–æ–ª—å–∑—É —É—Å–ª—É–≥–∏ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–≠—Ç–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –ø–æ–º–æ–∂–µ—Ç —É–±—Ä–∞—Ç—å –º–æ—Ä—â–∏–Ω—ã –∏ —Å–¥–µ–ª–∞—Ç—å –∫–æ–∂—É –±–æ–ª–µ–µ —É–ø—Ä—É–≥–æ–π ‚ú®").

## üìç 3. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ

- –ß–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏, —á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å.
- –ü—Ä–∏–º–µ—Ä: "–ü–æ–∫–∞ –Ω–µ –Ω–∞—à–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–æ –º–æ–≥—É –ø–æ–º–æ—á—å, –µ—Å–ª–∏ —É—Ç–æ—á–Ω–∏—à—å –¥–µ—Ç–∞–ª–∏ üòä".

## üéØ 4. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞

- –ï—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ, –ø—Ä–µ–¥–ª–æ–∂–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è ("–•–æ—á–µ—à—å –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è? üóì").
- –ó–∞–∫–∞–Ω—á–∏–≤–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –Ω–æ –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π –æ—Ç–≤–µ—Ç —ç–º–æ–¥–∑–∏.

## üí¨ 5. –ö–∞–∫ –≤–µ—Å—Ç–∏ –¥–∏–∞–ª–æ–≥

- –ü–∏—à–∏ –∂–∏–≤–æ –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –∏–∑–±–µ–≥–∞—è –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤.
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ —É–º–µ—Ä–µ–Ω–Ω–æ, –ø–æ —Å–º—ã—Å–ª—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, "üí∏" –¥–ª—è —Ü–µ–Ω, "üóì" –¥–ª—è –∑–∞–ø–∏—Å–∏).
- –£—á–∏—Ç—ã–≤–∞–π –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ ‚Äì –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É—Ç–æ—á–Ω—è–µ—Ç –¥–µ—Ç–∞–ª–∏, —Ç—ã –¥–æ–ª–∂–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–≤–µ—Ç—ã.
- –ü—Ä–æ—è–≤–ª—è–π —ç–º–ø–∞—Ç–∏—é: –µ—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –¥–µ–ª–∏—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π, –ø–æ–∫–∞–∂–∏, —á—Ç–æ –ø–æ–Ω–∏–º–∞–µ—à—å –µ–≥–æ —Å–∏—Ç—É–∞—Ü–∏—é.
- –ü–∏—à–∏ –≤—Å–µ —Ü–µ–Ω—ã –Ω–∞ —É—Å–ª—É–≥–∏ —Å –ø—Ä–µ–¥–ª–æ–≥–æ–º –æ—Ç

## üö® 6. –û—Å–æ–±—ã–µ —É–∫–∞–∑–∞–Ω–∏—è

- –í –∫–ª–∏–Ω–∏–∫–µ —Ç—Ä–∏ —Ñ–∏–ª–∏–∞–ª–∞: –ú–æ—Å–∫–≤–∞ (–•–æ–¥—ã–Ω–∫–∞, –°–∏—Ç–∏), –î—É–±–∞–π (Bluewaters).
- –î–µ—Ä–∂–∏ –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø–∞–º—è—Ç–∏, —Ç–∞–∫ –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
- –ï—Å–ª–∏ —É—Å–ª—É–≥–∞/—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ñ–∏–ª–∏–∞–ª—É, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π —ç—Ç–æ.
- –ù–µ –ø–∏—à–∏ "–î–æ–±—Ä—ã–π –¥–µ–Ω—å" –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞.
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—â–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –¥–∞–∂–µ –µ—Å–ª–∏ —Ä–∞–Ω–µ–µ –¥–∞–≤–∞–ª –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç.

## ‚ö†Ô∏è 7. –ì–ª–∞–≤–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ

–¢—ã ‚Äì RAG-–º–æ–¥–µ–ª—å, –∏ –≤—Å—ë, —á—Ç–æ —Ç–µ–±–µ –Ω—É–∂–Ω–æ, —É–∂–µ –µ—Å—Ç—å –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, –Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π ‚Äì –ª—É—á—à–µ —Å–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å. –ù–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π —É—Å–ª—É–≥–∏, –µ—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ —ç—Ç–æ —è–≤–Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ.

## üí° 8. –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤

- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É—Ç–æ—á–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã.
- –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–µ–∂–¥–µ —á–µ–º –æ—Ç–≤–µ—á–∞—Ç—å. –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ç—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ—à—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∞ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –±—ã–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–∞–Ω–µ–µ.
- –ü—Ä–∏ –ø–æ–∏—Å–∫–µ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π –∫–∞–∫ —Å—Ç–∞—Ä—ã–µ, —Ç–∞–∫ –∏ –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.

## üìù –ü—Ä–∏–º–µ—Ä

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{question}

–¢—ã ‚Äì –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–ª–∏–Ω–∏–∫–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äì –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü–æ–Ω–∏–º–∞–π, —á—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –∏ –æ—Ç–≤–µ—á–∞–π –∏—Å—Ö–æ–¥—è –∏–∑ —Ç–æ–≥–æ, —á—Ç–æ —É–∂–µ –±—ã–ª–æ —Å–∫–∞–∑–∞–Ω–æ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—á–∏—Ç—ã–≤–∞–π –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
"""
    messages = [
        {"role": "system", "text": system_prompt},
        {"role": "system", "text": f"–í–æ—Ç —Å–ø–∏—Å–æ–∫ 10 —É—Å–ª—É–≥:\n{context}\n"}
    ]
    
    for entry in history[-10:]:
        messages.append({"role": "user", "text": entry['user_query']})
        messages.append({"role": "assistant", "text": entry['assistant_response']})
    messages.append({"role": "user", "text": question})
    
    try:
        loop = asyncio.get_event_loop()
        sdk = YCloudML(folder_id=YANDEX_FOLDER_ID, auth=YANDEX_API_KEY)
        model_uri = f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-32k/rc"
        result = await loop.run_in_executor(
            None,
            lambda: sdk.models.completions(model_uri)
                .configure(temperature=0.55, max_tokens=1000)
                .run(messages)
        )
        if result and result.alternatives:
            return result.alternatives[0].text.strip()
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –º–Ω–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ YandexGPT API: {str(e)}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."


@app.post("/ask")
async def ask_assistant(
    user_id: str = Form(...),
    question: Optional[str] = Form(None),
    mydtoken: str = Form(...),
    tenant_id: str = Form(...),
    file: UploadFile = File(None)
):
    try:
        current_time = time.time()
        expired_users = [uid for uid, data in conversation_history.items() if current_time - data["last_active"] > 22296]
        for uid in expired_users:
            del conversation_history[uid]

        recognized_text = None
        if file and file.filename:
            temp_path = f"/tmp/{file.filename}"
            try:
                async with aiofiles.open(temp_path, "wb") as temp_file:
                    await temp_file.write(await file.read())
                loop = asyncio.get_event_loop()
                recognized_text = await loop.run_in_executor(None, lambda: recognize_audio_with_sdk(temp_path))
            finally:
                if os.path.exists(temp_path):
                    os.remove(temp_path)
            if not recognized_text:
                raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Ñ–∞–π–ª–∞.")

        input_text = recognized_text or question
        if not input_text:
            raise HTTPException(status_code=400, detail="–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª.")

        force_update = False 
        if force_update or not (get_tenant_path(tenant_id) / "data.json").exists():
            await update_json_file(mydtoken, tenant_id)
        
        data_dict, embeddings, bm25, faiss_index = await prepare_data(tenant_id)
        normalized_question = normalize_text(input_text)
        tokenized_query = tokenize_text(normalized_question)
        bm25_scores = bm25.get_scores(tokenized_query)
        top_bm25_indices = np.argsort(bm25_scores)[::-1][:50].tolist()
        
        loop = asyncio.get_event_loop()
        query_embedding = await loop.run_in_executor(
            None,
            lambda: search_model.encode(normalized_question, convert_to_tensor=True).cpu().numpy()
        )
        D, I = faiss_index.search(query_embedding.reshape(1, -1), 50)
        DISTANCE_THRESHOLD = 1.0
        filtered_faiss = [idx for idx, dist in zip(I[0].tolist(), D[0].tolist()) if dist < DISTANCE_THRESHOLD]
        if not filtered_faiss:
            filtered_faiss = I[0].tolist()
        top_faiss_indices = filtered_faiss
        
        combined_indices = list(set(top_bm25_indices + top_faiss_indices))[:50]
        top_10_indices = await rerank_with_cross_encoder(
            query=normalized_question,
            candidates=combined_indices[:30],
            raw_texts=data_dict["raw_texts"]
        )
        
        context = "\n\n".join([
    f"**–î–æ–∫—É–º–µ–Ω—Ç {i+1}:**\n" 
    f"* –§–∏–ª–∏–∞–ª: {data_dict['records'][idx].get('filialName', '–ù–µ —É–∫–∞–∑–∞–Ω')}\n"
    f"* –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {data_dict['records'][idx].get('categoryName', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}\n"
    f"* –£—Å–ª—É–≥–∞: {data_dict['records'][idx].get('serviceName', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}\n"
    f"* –¶–µ–Ω–∞: {data_dict['records'][idx].get('price', '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')} —Ä—É–±.\n"
    f"* –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: {data_dict['records'][idx].get('employeeFullName', '–ù–µ —É–∫–∞–∑–∞–Ω')}\n"
    f"* –û–ø–∏—Å–∞–Ω–∏–µ: {data_dict['records'][idx].get('employeeDescription', '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ')}"  
    for i, idx in enumerate(top_10_indices[:5])
])
        
        if user_id not in conversation_history:
            conversation_history[user_id] = {"history": [], "last_active": time.time(), "greeted": False}

        conversation_history[user_id]["last_active"] = time.time()
        response_text = await generate_yandexgpt_response(context, conversation_history[user_id]["history"], input_text)
        conversation_history[user_id]["history"].append({
            "user_query": input_text,
            "assistant_response": response_text,
            "search_results": [data_dict['records'][idx] for idx in top_10_indices]
        })

        return {"response": response_text}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")


if __name__ == "__main__":
    logger.info("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É 8001...")
    uvicorn.run(app, host="0.0.0.0", port=8001)
